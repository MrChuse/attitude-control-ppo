{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c0c810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78aa170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n",
    "\n",
    "\n",
    "def mlp(x, sizes, activation=tf.nn.relu, output_activation=None):\n",
    "    # Build a feedforward neural network\n",
    "    for size in sizes[:-1]:\n",
    "        x = layers.Dense(units=size, activation=activation)(x)\n",
    "    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, num_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "):\n",
    "\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = tf.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = tf.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -tf.reduce_mean(\n",
    "            tf.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = tf.reduce_mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = tf.reduce_sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = tf.reduce_mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78fd5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_euler_angles(t, roll, pitch, yaw, filename=None, bbox=(60,100,-1,1), inset=True):\n",
    "    fig, ax = plt.subplots(figsize=(16,8))\n",
    "    ax.set_title(\"Euler Angles\")\n",
    "    \n",
    "    ax.plot(t, roll, label = 'roll', color = 'red')\n",
    "    ax.plot(t, pitch, label = 'pitch', color = 'green')\n",
    "    ax.plot(t, yaw, label = 'yaw', color = 'blue')\n",
    "    \n",
    "    if inset:\n",
    "        ylims = ax.get_ylim()\n",
    "        above = abs(ylims[1]) > abs(ylims[0])\n",
    "        axins = ax.inset_axes([0.3, 0.55 if above else 0.05 , 0.5, 0.25])\n",
    "        x1, x2, y1, y2 = bbox\n",
    "        axins.set_xlim(x1, x2)\n",
    "        axins.set_ylim(y1, y2)\n",
    "        axins.plot(t, roll, label = 'roll', color = 'red')\n",
    "        axins.plot(t, pitch, label = 'pitch', color = 'green')\n",
    "        axins.plot(t, yaw, label = 'yaw', color = 'blue')\n",
    "        axins.set_xticklabels([])\n",
    "        axins.grid()\n",
    "        ax.indicate_inset_zoom(axins, edgecolor=\"black\")\n",
    "\n",
    "    ax.set_ylabel(r'angles, [deg]')\n",
    "    ax.set_xlabel(r't, [s]')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    fig.show()\n",
    "    if filename is not None:\n",
    "        fig.savefig(f'{filename}')\n",
    "\n",
    "def plot_phase_diagram(roll, pitch, yaw, omega_, filename=None):\n",
    "    fig2 = plt.figure(figsize=(8,8))\n",
    "    ax2 = fig2.add_subplot(1,1,1)\n",
    "    ax2.set_title(\"Phase diagram\")\n",
    "    ax2.plot(roll, omega_[0], label = 'roll', color = 'red')\n",
    "    ax2.plot(pitch, omega_[1], label = 'pitch', color = 'green')\n",
    "    ax2.plot(yaw, omega_[2], label = 'yaw', color = 'blue')\n",
    "    ax2.plot([0],[0], 'o', markersize=8 , label='Destination', color = 'magenta')\n",
    "    ax2.set_xlabel(r'$\\alpha$')\n",
    "    ax2.set_ylabel(r'$\\omega$')\n",
    "    ax2.legend()\n",
    "    ax2.grid()\n",
    "    fig2.show()\n",
    "    if filename is not None:\n",
    "        fig2.savefig(f'{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d94b628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(obj):\n",
    "\n",
    "    return obj / np.linalg.norm(obj)\n",
    "\n",
    "\n",
    "def cross_product(a, b):\n",
    "\n",
    "    def check_dimensions(vec, string):\n",
    "\n",
    "        if vec.ndim != 1:\n",
    "            raise Exception(\"The {} input is not a vector\".format(string))\n",
    "        if len(vec) != 3:\n",
    "            raise Exception(\"Wrong number of coordinates in the {0} vector: {1}, should be 3\".format(string, len(vec)))\n",
    "\n",
    "    check_dimensions(a, 'first')\n",
    "    check_dimensions(b, 'second')\n",
    "\n",
    "    return np.array([a[1]*b[2]-a[2]*b[1], a[2]*b[0]-a[0]*b[2], a[0]*b[1] - a[1]*b[0]])\n",
    "\n",
    "def quat_product(q1, q2):\n",
    "\n",
    "    def check_dimensions(q, string):\n",
    "\n",
    "        if q.ndim != 1:\n",
    "            raise Exception(\"The {} input is not a quaternion\".format(string))\n",
    "        if len(q) != 4:\n",
    "            raise Exception(\"Wrong number of coordinates in the {0} quaternion: {1}, should be 4\".format(string, len(q)))\n",
    "\n",
    "    check_dimensions(q1, 'first')\n",
    "    check_dimensions(q2, 'second')\n",
    "\n",
    "    q = np.zeros(4)\n",
    "    q[0] = q1[0] * q2[0] - q1[1:].dot(q2[1:])\n",
    "    q[1:] = q1[0] * q2[1:] + q2[0] * q1[1:] + cross_product(q1[1:], q2[1:])\n",
    "\n",
    "    return q\n",
    "\n",
    "def rotate_vec_with_quat(q, vec):\n",
    "\n",
    "    def check_dimensions(obj, is_quat):\n",
    "\n",
    "        if obj.ndim != 1:\n",
    "            raise Exception(\"Not a {}\".format('quaternion' * is_quat + 'vector' * (1 - is_quat)))\n",
    "        if len(obj) != (3 + 1 * is_quat):\n",
    "            raise Exception(\"Wrong number of coordinates in the {0}: {1}, should be {2}\"\n",
    "                            .format('quaternion' * is_quat + 'vector' * (1 - is_quat), len(obj), 3 + 1 * is_quat))\n",
    "\n",
    "    check_dimensions(q, True)\n",
    "    check_dimensions(vec, False)\n",
    "\n",
    "    q = quat_conjugate(q)\n",
    "\n",
    "    qxvec = cross_product(q[1:], vec)\n",
    "\n",
    "    return q[1:].dot(vec) * q[1:] + q[0]**2. * vec + 2. * q[0] * qxvec + cross_product(q[1:], qxvec)\n",
    "\n",
    "def quat2rpy(q0, q1, q2, q3):\n",
    "\n",
    "    roll = np.arctan2(2. * (q0 * q1 + q2 * q3), 1. - 2. * (q1**2 + q2**2))\n",
    "    pitch = np.arcsin(2. * (q0 * q2 - q1 * q3))\n",
    "    yaw = np.arctan2(2. * (q0 * q3 + q1 * q2), 1. - 2. * (q2**2 + q3**2))\n",
    "\n",
    "    return [roll, pitch, yaw]\n",
    "\n",
    "def quat2rpy_deg(q0, q1, q2, q3):\n",
    "\n",
    "    roll = np.arctan2(2. * (q0 * q1 + q2 * q3), 1. - 2. * (q1**2 + q2**2))*180/np.pi\n",
    "    pitch = np.arcsin(2. * (q0 * q2 - q1 * q3))*180/np.pi\n",
    "    yaw = np.arctan2(2. * (q0 * q3 + q1 * q2), 1. - 2. * (q2**2 + q3**2))*180/np.pi\n",
    "\n",
    "    return [roll, pitch, yaw]\n",
    "\n",
    "def quat_conjugate(q):\n",
    "\n",
    "    q_new = np.copy(q)\n",
    "    q_new[1:] = q_new[1:] * -1.\n",
    "\n",
    "    return q_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41277379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, add_9, start=None):\n",
    "# Iterate over the number of epochs\n",
    "  print('starting training with', epochs, 'epochs')\n",
    "  buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
    "  env = TorqueDynamics(0.1, np.array([1, 0, 0, 0]), add_9)\n",
    "  # Initialize the observation, episode return and episode length\n",
    "  observation, episode_return, episode_length = env.reset(start), 0, 0\n",
    "\n",
    "  returns = []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "      # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "      sum_return = 0\n",
    "      sum_length = 0\n",
    "      num_episodes = 0\n",
    "\n",
    "      # Iterate over the steps of each epoch\n",
    "      for t in range(steps_per_epoch):\n",
    "          if render:\n",
    "              env.render()\n",
    "\n",
    "          # Get the logits, action, and take one step in the environment\n",
    "          observation = observation.reshape(1, -1)\n",
    "          logits, action = sample_action(observation)\n",
    "          #print(type(action[0].numpy()))\n",
    "          observation_new, reward, done, info = env.step(action[0].numpy())\n",
    "          episode_return += reward\n",
    "          episode_length += 1\n",
    "\n",
    "          # Get the value and log-probability of the action\n",
    "          value_t = critic(observation)\n",
    "          logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "          # Store obs, act, rew, v_t, logp_pi_t\n",
    "          buffer.store(observation, action, reward, value_t, logprobability_t)\n",
    "\n",
    "          # Update the observation\n",
    "          observation = observation_new\n",
    "\n",
    "          # Finish trajectory if reached to a terminal state\n",
    "          terminal = done\n",
    "          if terminal or (t == steps_per_epoch - 1):\n",
    "              last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "              buffer.finish_trajectory(last_value)\n",
    "              sum_return += episode_return\n",
    "              sum_length += episode_length\n",
    "              num_episodes += 1\n",
    "              observation, episode_return, episode_length = env.reset(start), 0, 0\n",
    "\n",
    "      # Get values from the buffer\n",
    "      (\n",
    "          observation_buffer,\n",
    "          action_buffer,\n",
    "          advantage_buffer,\n",
    "          return_buffer,\n",
    "          logprobability_buffer,\n",
    "      ) = buffer.get()\n",
    "\n",
    "      # Update the policy and implement early stopping using KL divergence\n",
    "      for _ in range(train_policy_iterations):\n",
    "          kl = train_policy(\n",
    "              observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "          )\n",
    "          if kl > 1.5 * target_kl:\n",
    "              # Early Stopping\n",
    "              break\n",
    "\n",
    "      # Update the value function\n",
    "      for _ in range(train_value_iterations):\n",
    "          train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "      # Print mean return and length for each epoch\n",
    "      print(\n",
    "          f\" Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\"\n",
    "      )\n",
    "      returns.append(sum_return / num_episodes)\n",
    "  return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26fb9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
